-- June Snippets: Projects & Professional Development
INSERT INTO snippets (month, day, title, code, explanation, language_id, category_id) VALUES
(6, 1, 'Git Workflow', '#!/bin/bash\n# Feature branch workflow\ngit checkout -b feature/new-feature\n# Make changes\ngit add -A\ngit commit -m "feat: add new feature"\n\n# Interactive rebase to clean history\ngit rebase -i main\n\n# Push and create PR\ngit push -u origin feature/new-feature\n\n# After review, squash merge\ngit checkout main\ngit merge --squash feature/new-feature\ngit commit -m "feat: implement complete feature"', 'Git workflow for teams. Feature branches isolate work. Interactive rebase cleans history. Squash merge keeps main clean. PR for code review.', 14, 7),
(6, 2, 'Docker Containerization', 'FROM node:16-alpine\nWORKDIR /app\n\n# Cache dependencies\nCOPY package*.json ./\nRUN npm ci --only=production\n\n# Copy app\nCOPY . .\n\n# Non-root user\nRUN addgroup -g 1001 -S nodejs\nRUN adduser -S nodejs -u 1001\nUSER nodejs\n\nEXPOSE 3000\nCMD ["node", "server.js"]\n\n# Build: docker build -t app .\n# Run: docker run -p 3000:3000 app', 'Docker packages apps with dependencies. Multi-stage builds reduce size. Layer caching speeds builds. Non-root for security. Portable deployment.', 16, 7),
(6, 3, 'CI/CD Pipeline', 'name: Deploy\non:\n  push:\n    branches: [main]\njobs:\n  test:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n      - run: npm ci\n      - run: npm test\n      - run: npm run build\n  \n  deploy:\n    needs: test\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n      - run: |\n          echo "${{ secrets.DEPLOY_KEY }}" | ssh-add -\n          ssh user@server "cd /app && git pull && npm install && pm2 restart app"', 'CI/CD automates deployment. Tests run on every push. Deploy only from main branch. Secrets store credentials. Zero-downtime deployment.', 13, 7),
(6, 4, 'Code Documentation', '/**\n * Calculates compound interest\n * @param {number} principal - Initial amount\n * @param {number} rate - Annual interest rate (decimal)\n * @param {number} time - Time period in years\n * @param {number} n - Compounding frequency per year\n * @returns {number} Final amount after compound interest\n * @example\n * // Returns 110.25 for 10% annual interest over 1 year\n * calculateCompoundInterest(100, 0.1, 1, 4)\n */\nfunction calculateCompoundInterest(principal, rate, time, n) {\n    return principal * Math.pow((1 + rate / n), n * time);\n}', 'JSDoc documents JavaScript code. Describes parameters, returns, examples. IDEs show hints. Generate docs automatically. Improves maintainability.', 5, 7),
(6, 5, 'API Design REST', 'from flask import Flask, jsonify, request\nfrom http import HTTPStatus\n\napp = Flask(__name__)\n\n# RESTful endpoints\n@app.route(''/api/v1/users'', methods=[''GET''])\ndef get_users():\n    return jsonify({''users'': users}), HTTPStatus.OK\n\n@app.route(''/api/v1/users/<int:user_id>'', methods=[''GET''])\ndef get_user(user_id):\n    user = find_user(user_id)\n    if not user:\n        return jsonify({''error'': ''Not found''}), HTTPStatus.NOT_FOUND\n    return jsonify(user), HTTPStatus.OK\n\n@app.route(''/api/v1/users'', methods=[''POST''])\ndef create_user():\n    return jsonify(user), HTTPStatus.CREATED', 'RESTful API design. HTTP verbs for actions. Status codes communicate results. Versioning for compatibility. JSON for data exchange.', 1, 11),
(6, 6, 'Database Migrations', '-- migrations/001_create_users.sql\nCREATE TABLE users (\n    id SERIAL PRIMARY KEY,\n    email VARCHAR(255) UNIQUE NOT NULL,\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n);\n\n-- migrations/002_add_user_profile.sql\nALTER TABLE users ADD COLUMN name VARCHAR(100);\nALTER TABLE users ADD COLUMN avatar_url TEXT;\n\n-- rollback/002_add_user_profile.sql\nALTER TABLE users DROP COLUMN name;\nALTER TABLE users DROP COLUMN avatar_url;\n\n-- Track: INSERT INTO migrations (version, applied_at)', 'Database migrations version schema changes. Sequential numbered files. Forward and rollback scripts. Track applied migrations. Team synchronization.', 10, 3),
(6, 7, 'Logging Best Practices', 'import logging\nimport json\nfrom datetime import datetime\n\nclass JSONFormatter(logging.Formatter):\n    def format(self, record):\n        log_obj = {\n            ''timestamp'': datetime.utcnow().isoformat(),\n            ''level'': record.levelname,\n            ''message'': record.getMessage(),\n            ''module'': record.module,\n            ''function'': record.funcName,\n            ''line'': record.lineno\n        }\n        if hasattr(record, ''user_id''):\n            log_obj[''user_id''] = record.user_id\n        return json.dumps(log_obj)\n\nlogger = logging.getLogger(__name__)\nlogger.info("User logged in", extra={''user_id'': 123})', 'Structured logging aids debugging. JSON format for parsing. Include context (user, request ID). Log levels: DEBUG, INFO, WARNING, ERROR. Centralized aggregation.', 1, 7),
(6, 8, 'Error Handling', 'class AppError extends Error {\n    constructor(message, statusCode, isOperational = true) {\n        super(message);\n        this.statusCode = statusCode;\n        this.isOperational = isOperational;\n        Error.captureStackTrace(this, this.constructor);\n    }\n}\n\nclass ValidationError extends AppError {\n    constructor(message) {\n        super(message, 400);\n    }\n}\n\n// Global error handler\napp.use((err, req, res, next) => {\n    logger.error(err);\n    if (!err.isOperational) {\n        process.exit(1);  // Unexpected errors\n    }\n    res.status(err.statusCode || 500).json({\n        error: err.message\n    });\n});', 'Custom error classes improve handling. Operational vs programmer errors. Centralized error handling. Log all errors. Graceful degradation.', 5, 7),
(6, 9, 'Environment Configuration', '# .env.example\nNODE_ENV=development\nPORT=3000\nDATABASE_URL=postgresql://user:pass@localhost/db\nREDIS_URL=redis://localhost:6379\nJWT_SECRET=change-this-secret\nAWS_ACCESS_KEY_ID=your-key\nAWS_SECRET_ACCESS_KEY=your-secret\n\n# config.js\nrequire(''dotenv'').config();\n\nmodule.exports = {\n    port: process.env.PORT || 3000,\n    db: {\n        url: process.env.DATABASE_URL,\n        pool: { min: 2, max: 10 }\n    },\n    jwt: {\n        secret: process.env.JWT_SECRET,\n        expiresIn: ''1d''\n    }\n};', 'Environment variables configure apps. Never commit secrets. .env.example documents required vars. Different configs per environment. Validate on startup.', 5, 7),
(6, 10, 'Code Review Checklist', '# Code Review Checklist\n\n## Functionality\n- [ ] Code does what it should\n- [ ] Edge cases handled\n- [ ] Error handling present\n\n## Code Quality\n- [ ] Clear variable/function names\n- [ ] No code duplication\n- [ ] Follows style guide\n- [ ] Complexity reasonable\n\n## Testing\n- [ ] Unit tests included\n- [ ] Tests pass locally\n- [ ] Coverage adequate\n\n## Security\n- [ ] Input validated\n- [ ] No hardcoded secrets\n- [ ] SQL injection prevented\n\n## Performance\n- [ ] No obvious bottlenecks\n- [ ] Database queries optimized', 'Code reviews improve quality. Checklist ensures consistency. Look for bugs, style, security. Constructive feedback. Knowledge sharing opportunity.', 16, 7),
(6, 11, 'Microservices Communication', 'import grpc\nimport service_pb2\nimport service_pb2_grpc\n\n# gRPC server\nclass UserService(service_pb2_grpc.UserServiceServicer):\n    def GetUser(self, request, context):\n        user = find_user(request.id)\n        return service_pb2.User(\n            id=user.id,\n            name=user.name,\n            email=user.email\n        )\n\n# gRPC client\nchannel = grpc.insecure_channel(''localhost:50051'')\nstub = service_pb2_grpc.UserServiceStub(channel)\nresponse = stub.GetUser(service_pb2.GetUserRequest(id=123))', 'Microservices need efficient communication. gRPC uses Protocol Buffers. Binary format, strongly typed. Faster than REST. Streaming support.', 1, 11),
(6, 12, 'Caching Strategy', 'import redis\nimport hashlib\nimport json\n\ncache = redis.Redis()\n\ndef cached(expiration=3600):\n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            # Create cache key\n            key = f"{func.__name__}:{hashlib.md5(\n                json.dumps([args, kwargs]).encode()\n            ).hexdigest()}"\n            \n            # Check cache\n            result = cache.get(key)\n            if result:\n                return json.loads(result)\n            \n            # Compute and cache\n            result = func(*args, **kwargs)\n            cache.setex(key, expiration, json.dumps(result))\n            return result\n        return wrapper\n    return decorator', 'Caching improves performance. Redis for distributed cache. Cache key from function and args. TTL prevents stale data. Invalidation strategy crucial.', 1, 10),
(6, 13, 'API Rate Limiting', 'class TokenBucket {\n    constructor(capacity, refillRate) {\n        this.capacity = capacity;\n        this.tokens = capacity;\n        this.refillRate = refillRate;\n        this.lastRefill = Date.now();\n    }\n    \n    consume(tokens = 1) {\n        this.refill();\n        if (this.tokens >= tokens) {\n            this.tokens -= tokens;\n            return true;\n        }\n        return false;\n    }\n    \n    refill() {\n        const now = Date.now();\n        const timePassed = (now - this.lastRefill) / 1000;\n        this.tokens = Math.min(\n            this.capacity,\n            this.tokens + timePassed * this.refillRate\n        );\n        this.lastRefill = now;\n    }\n}', 'Token bucket algorithm for rate limiting. Smooth traffic flow. Allows bursts within capacity. Refills at constant rate. Per-user buckets recommended.', 5, 11),
(6, 14, 'Database Connection Pool', 'const { Pool } = require(''pg'');\n\nconst pool = new Pool({\n    host: process.env.DB_HOST,\n    database: process.env.DB_NAME,\n    user: process.env.DB_USER,\n    password: process.env.DB_PASSWORD,\n    max: 20,  // Maximum connections\n    idleTimeoutMillis: 30000,\n    connectionTimeoutMillis: 2000,\n});\n\n// Use pool\nasync function query(text, params) {\n    const start = Date.now();\n    const res = await pool.query(text, params);\n    const duration = Date.now() - start;\n    console.log(''Query took'', duration, ''ms'');\n    return res;\n}', 'Connection pools reuse database connections. Reduces overhead of creating connections. Configure pool size carefully. Monitor pool metrics. Essential for performance.', 5, 3),
(6, 15, 'Message Queue', 'import pika\nimport json\n\n# Producer\nconnection = pika.BlockingConnection(\n    pika.ConnectionParameters(''localhost''))\nchannel = connection.channel()\nchannel.queue_declare(queue=''tasks'', durable=True)\n\nmessage = json.dumps({''action'': ''send_email'', ''to'': ''user@example.com''})\nchannel.basic_publish(\n    exchange='''',\n    routing_key=''tasks'',\n    body=message,\n    properties=pika.BasicProperties(delivery_mode=2)  # Persistent\n)\n\n# Consumer\ndef callback(ch, method, properties, body):\n    task = json.loads(body)\n    process_task(task)\n    ch.basic_ack(delivery_tag=method.delivery_tag)\n\nchannel.basic_consume(queue=''tasks'', on_message_callback=callback)', 'Message queues decouple services. RabbitMQ, Redis, Kafka options. Async processing. Handles load spikes. Retry failed tasks. Scalable architecture.', 1, 11),
(6, 16, 'Monitoring & Metrics', 'const prometheus = require(''prom-client'');\n\n// Metrics\nconst httpDuration = new prometheus.Histogram({\n    name: ''http_request_duration_seconds'',\n    help: ''Duration of HTTP requests in seconds'',\n    labelNames: [''method'', ''route'', ''status'']\n});\n\nconst activeConnections = new prometheus.Gauge({\n    name: ''active_connections'',\n    help: ''Number of active connections''\n});\n\n// Middleware\napp.use((req, res, next) => {\n    const end = httpDuration.startTimer();\n    res.on(''finish'', () => {\n        end({ method: req.method, route: req.path, status: res.statusCode });\n    });\n    next();\n});\n\n// Metrics endpoint\napp.get(''/metrics'', (req, res) => {\n    res.set(''Content-Type'', prometheus.register.contentType);\n    res.end(prometheus.register.metrics());\n});', 'Monitoring tracks system health. Prometheus collects metrics. Histograms for latency. Gauges for current values. Grafana visualizes. Alerts on thresholds.', 5, 7),
(6, 17, 'Feature Flags', 'class FeatureFlags {\n    constructor() {\n        this.flags = new Map();\n    }\n    \n    set(feature, config) {\n        this.flags.set(feature, config);\n    }\n    \n    isEnabled(feature, user = null) {\n        const config = this.flags.get(feature);\n        if (!config) return false;\n        \n        if (config.enabled === false) return false;\n        if (config.enabled === true) return true;\n        \n        // Percentage rollout\n        if (config.percentage && user) {\n            const hash = this.hash(feature + user.id);\n            return (hash % 100) < config.percentage;\n        }\n        \n        // User whitelist\n        if (config.users && user) {\n            return config.users.includes(user.id);\n        }\n        \n        return false;\n    }\n}', 'Feature flags control functionality. Gradual rollouts. A/B testing. Quick rollback. Decouple deployment from release. Essential for continuous delivery.', 5, 7),
(6, 18, 'Semantic Versioning', '{\n  "name": "my-package",\n  "version": "2.1.3",\n  // MAJOR.MINOR.PATCH\n  // MAJOR: Breaking changes\n  // MINOR: New features, backwards compatible\n  // PATCH: Bug fixes\n  \n  "dependencies": {\n    "express": "^4.17.1",  // Compatible with 4.x.x\n    "lodash": "~4.17.0",   // Compatible with 4.17.x\n    "uuid": "8.3.2"        // Exact version\n  },\n  \n  "scripts": {\n    "version": "npm run build && git add -A dist",\n    "postversion": "git push && git push --tags"\n  }\n}\n\n// npm version patch  # 2.1.3 -> 2.1.4\n// npm version minor  # 2.1.3 -> 2.2.0\n// npm version major  # 2.1.3 -> 3.0.0', 'Semantic versioning communicates changes. Major.Minor.Patch format. Caret (^) allows minor updates. Tilde (~) allows patches. Lock files ensure reproducibility.', 16, 7),
(6, 19, 'Database Indexing', 'CREATE INDEX idx_users_email ON users(email);\nCREATE INDEX idx_posts_user_created \n    ON posts(user_id, created_at DESC);\nCREATE INDEX idx_products_search \n    ON products USING GIN(to_tsvector(''english'', name || '' '' || description));\n\n-- Analyze query performance\nEXPLAIN ANALYZE\nSELECT * FROM posts\nWHERE user_id = 123\nORDER BY created_at DESC\nLIMIT 10;\n\n-- Find missing indexes\nSELECT schemaname, tablename, attname, n_distinct, correlation\nFROM pg_stats\nWHERE schemaname = ''public''\nAND n_distinct > 100\nAND correlation < 0.1\nORDER BY n_distinct DESC;', 'Indexes speed up queries. B-tree default, GIN for full-text. Composite indexes for multiple columns. EXPLAIN shows query plan. Balance read/write performance.', 10, 3),
(6, 20, 'Deployment Strategies', '# Blue-Green Deployment\n# Two identical environments\n# Switch traffic after testing\n\n# Rolling Deployment\nkubectl set image deployment/app app=app:v2\nkubectl rollout status deployment/app\n\n# Canary Deployment\napiVersion: v1\nkind: Service\nspec:\n  selector:\n    app: myapp\n  # 90% to stable, 10% to canary\n  sessionAffinity: ClientIP\n\n# Feature Flags\nif (featureFlag.isEnabled("new-algorithm")) {\n    return newAlgorithm();\n} else {\n    return oldAlgorithm();\n}', 'Deployment strategies minimize risk. Blue-green for instant rollback. Rolling for gradual update. Canary tests with real traffic. Feature flags for granular control.', 13, 7),
(6, 21, 'Performance Optimization', 'use std::collections::HashMap;\nuse rayon::prelude::*;\n\n// Parallel processing\nfn process_data(items: Vec<Item>) -> Vec<Result> {\n    items.par_iter()\n        .map(|item| expensive_computation(item))\n        .collect()\n}\n\n// Memoization\nstruct Fibonacci {\n    cache: HashMap<u64, u64>,\n}\n\nimpl Fibonacci {\n    fn calculate(&mut self, n: u64) -> u64 {\n        if let Some(&result) = self.cache.get(&n) {\n            return result;\n        }\n        let result = match n {\n            0 | 1 => n,\n            _ => self.calculate(n - 1) + self.calculate(n - 2),\n        };\n        self.cache.insert(n, result);\n        result\n    }\n}', 'Optimization techniques: parallelization, memoization, caching. Profile first, optimize bottlenecks. Measure improvements. Premature optimization harmful.', 7, 10),
(6, 22, 'Security Audit', '#!/bin/bash\n# Security audit script\n\necho "=== Security Audit ==="\n\n# Check for exposed secrets\necho "Checking for secrets..."\ngit secrets --scan\n\n# Dependency vulnerabilities\necho "Checking dependencies..."\nnpm audit\npip-audit\n\n# OWASP dependency check\ndependency-check --scan . --format HTML\n\n# Static analysis\necho "Running static analysis..."\nsemgrep --config=auto .\nbandit -r python_code/\n\n# Container scanning\ntrivy image myapp:latest\n\n# SSL/TLS check\nnmap --script ssl-enum-ciphers -p 443 example.com', 'Security audits find vulnerabilities. Check dependencies, secrets, code patterns. Automated scanning tools. Regular audits essential. Fix critical issues first.', 14, 8),
(6, 23, 'GraphQL Schema', 'type Query {\n  user(id: ID!): User\n  users(limit: Int = 10, offset: Int = 0): [User!]!\n}\n\ntype Mutation {\n  createUser(input: CreateUserInput!): User!\n  updateUser(id: ID!, input: UpdateUserInput!): User!\n}\n\ntype User {\n  id: ID!\n  name: String!\n  email: String!\n  posts: [Post!]!\n  createdAt: DateTime!\n}\n\ninput CreateUserInput {\n  name: String!\n  email: String!\n}\n\ntype Subscription {\n  userUpdated(id: ID!): User!\n}', 'GraphQL schema defines API. Strong typing. Query exactly needed data. Mutations modify. Subscriptions for real-time. Single endpoint. Client-driven.', 16, 11),
(6, 24, 'Kubernetes Deployment', 'apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: app\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: myapp\n  template:\n    metadata:\n      labels:\n        app: myapp\n    spec:\n      containers:\n      - name: app\n        image: myapp:latest\n        ports:\n        - containerPort: 3000\n        env:\n        - name: DATABASE_URL\n          valueFrom:\n            secretKeyRef:\n              name: db-secret\n              key: url\n        resources:\n          limits:\n            memory: "256Mi"\n            cpu: "500m"\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 3000', 'Kubernetes orchestrates containers. Deployments manage pods. Auto-scaling, self-healing. Secrets for sensitive data. Resource limits prevent noisy neighbors.', 13, 7),
(6, 25, 'Technical Debt', '// TODO: Refactor this function - too complex\n// FIXME: Handle edge case when array is empty\n// HACK: Temporary workaround for API bug\n// DEPRECATED: Use newFunction() instead\n\n/**\n * @deprecated Since version 2.0. Will be removed in 3.0.\n * Use {@link newFunction} instead.\n */\nfunction oldFunction() {\n    console.warn("oldFunction is deprecated");\n    return newFunction();\n}\n\n// Technical Debt Register:\n// 1. Database queries need optimization (HIGH)\n// 2. No input validation in API (CRITICAL)\n// 3. Missing unit tests (MEDIUM)\n// 4. Hardcoded configuration (LOW)', 'Technical debt accumulates over time. Document with TODOs, FIXMEs. Deprecation warnings for migrations. Regular refactoring sessions. Balance features with maintenance.', 5, 7),
(6, 26, 'Load Balancer Config', 'upstream backend {\n    least_conn;\n    server backend1.example.com:8080 weight=5;\n    server backend2.example.com:8080 weight=3;\n    server backend3.example.com:8080 backup;\n    \n    keepalive 32;\n}\n\nserver {\n    listen 80;\n    server_name api.example.com;\n    \n    location / {\n        proxy_pass http://backend;\n        proxy_http_version 1.1;\n        proxy_set_header Upgrade $http_upgrade;\n        proxy_set_header Connection "upgrade";\n        proxy_set_header Host $host;\n        proxy_set_header X-Real-IP $remote_addr;\n        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n        proxy_set_header X-Forwarded-Proto $scheme;\n        \n        proxy_connect_timeout 60s;\n        proxy_send_timeout 60s;\n        proxy_read_timeout 60s;\n    }\n}', 'Load balancers distribute traffic. Least connections, round robin, IP hash algorithms. Health checks detect failures. Sticky sessions when needed.', 15, 11),
(6, 27, 'Project Structure', 'project/\n├── src/\n│   ├── controllers/    # Request handlers\n│   ├── services/       # Business logic\n│   ├── models/         # Data models\n│   ├── repositories/   # Data access\n│   ├── middleware/     # Express middleware\n│   ├── utils/          # Helper functions\n│   └── config/         # Configuration\n├── tests/\n│   ├── unit/\n│   ├── integration/\n│   └── e2e/\n├── scripts/            # Build, deploy scripts\n├── docs/               # Documentation\n├── .env.example\n├── docker-compose.yml\n├── Dockerfile\n└── README.md', 'Good project structure improves maintainability. Separation of concerns. Clear naming conventions. Consistent organization. Easy onboarding.', 16, 7),
(6, 28, 'Makefile Automation', 'SHELL := /bin/bash\n.PHONY: help build test deploy\n\nhelp: ## Show this help\n\t@grep -E ''^[a-zA-Z_-]+:.*?## .*$$'' $(MAKEFILE_LIST) | \\\n\t\tawk ''BEGIN {FS = ":.*?## "}; {printf "%-15s %s\\n", $$1, $$2}''\n\nbuild: ## Build the application\n\tdocker build -t app:latest .\n\ntest: ## Run tests\n\t@echo "Running tests..."\n\tnpm test\n\t@echo "Running linter..."\n\tnpm run lint\n\ndeploy: build test ## Deploy to production\n\t@echo "Deploying to production..."\n\tkubectl apply -f k8s/\n\tkubectl rollout status deployment/app\n\nclean: ## Clean build artifacts\n\trm -rf dist/ node_modules/', 'Makefiles automate tasks. Targets with dependencies. PHONY for non-file targets. Self-documenting with help. Standard interface across projects.', 16, 7),
(6, 29, 'Performance Profiling', 'import cProfile\nimport pstats\nfrom memory_profiler import profile\n\n# CPU profiling\ndef profile_function():\n    profiler = cProfile.Profile()\n    profiler.enable()\n    \n    # Code to profile\n    result = expensive_function()\n    \n    profiler.disable()\n    stats = pstats.Stats(profiler)\n    stats.sort_stats(''cumulative'')\n    stats.print_stats(10)  # Top 10 functions\n    \n    return result\n\n# Memory profiling\n@profile\ndef memory_intensive():\n    large_list = [i for i in range(1000000)]\n    large_dict = {i: i**2 for i in range(100000)}\n    return len(large_list) + len(large_dict)\n\n# Line profiler: kernprof -l -v script.py', 'Profiling identifies bottlenecks. CPU profiling shows time per function. Memory profiling tracks allocations. Line profiler for granular analysis.', 1, 10),
(6, 30, 'Summer Internship Tips', '# Epitech Summer Internship Success Guide\n\n## Technical Preparation\n- Master Git workflows\n- Practice code reviews\n- Learn Docker basics\n- Understand CI/CD\n\n## Soft Skills\n- Ask questions early\n- Document your work\n- Communicate progress\n- Take initiative\n\n## First Day\ngit clone <repo>\ndocker-compose up\nnpm install\nnpm test\n\n## Daily Routine\n- Morning standup\n- Code, test, commit\n- Afternoon review\n- Update tickets\n\nRemember: Every expert was once a beginner!', 'Internship success tips for Epitech students. Technical and soft skills equally important. Be proactive, ask questions. Document everything. Network and learn!', 16, 7);